{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('gan-3d-s': conda)"
  },
  "interpreter": {
   "hash": "da2b86edc79f11ef65626cb6dd06348ac67b7ad31f2e43ac8279b63ad866aaf3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Utils"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as K\n",
    "from keras.utils import conv_utils\n",
    "from keras.layers.convolutional import UpSampling3D\n",
    "from keras.engine import InputSpec\n",
    "from tensorlayer.layers import *\n",
    "import tensorflow as tf\n",
    "\n",
    "class UpSampling3D(Layer):\n",
    "    def __init__(self, size=(2, 2, 2), **kwargs):\n",
    "        self.size = conv_utils.normalize_tuple(size, 3, 'size')\n",
    "        self.input_spec = InputSpec(ndim=5)\n",
    "        super(UpSampling3D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        dim1 = self.size[0] * input_shape[1] if input_shape[1] is not None else None\n",
    "        dim2 = self.size[1] * input_shape[2] if input_shape[2] is not None else None\n",
    "        dim3 = self.size[2] * input_shape[3] if input_shape[3] is not None else None\n",
    "        return (input_shape[0],\n",
    "                dim1,\n",
    "                dim2,\n",
    "                dim3,\n",
    "                input_shape[4])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return K.resize_volumes(inputs,\n",
    "                                self.size[0], self.size[1], self.size[2],\n",
    "                                self.data_format)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'size': self.size,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(UpSampling3D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "def smooth_gan_labels(y):\n",
    "    if y == 0:\n",
    "        y_out = tf.random_uniform(shape=y.get_shape(), minval=0.0, maxval=0.3)\n",
    "    else:\n",
    "        y_out = tf.random_uniform(shape=y.get_shape(), minval=0.7, maxval=1.2)\n",
    "\n",
    "    return y_out\n",
    "\n",
    "\n",
    "def subPixelConv3d(net, img_width, img_height, img_depth, stepsToEnd, n_out_channel):\n",
    "    i = net\n",
    "    r = 2\n",
    "    a, b, z, c = int(img_width / (2 * stepsToEnd)), int(img_height / (2 * stepsToEnd)), int(\n",
    "        img_depth / (2 * stepsToEnd)), tf.shape(i)[3]\n",
    "    bsize = tf.shape(i)[0]  # Handling Dimension(None) type for undefined batch dim\n",
    "    xs = tf.split(i, r, 4)  # b*h*w*d*r*r*r\n",
    "    xr = tf.concat(xs, 3)  # b*h*w*(r*d)*r*r\n",
    "    xss = tf.split(xr, r, 4)  # b*h*w*(r*d)*r*r\n",
    "    xrr = tf.concat(xss, 2)  # b*h*(r*w)*(r*d)*r\n",
    "    x = tf.reshape(xrr, (bsize, r * a, r * b, r * z, n_out_channel))  # b*(r*h)*(r*w)*(r*d)*n_out n_out=64/2^\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def aggregate(patches):\n",
    "    margin = 16\n",
    "    volume = np.empty([224, 224, 152, 1])\n",
    "    volume[0:112, 0:112, 0:76, :] = patches[0, 0:112, 0:112, 0:76, :]\n",
    "    volume[0:112, 0:112, 76:, :] = patches[1, 0:112, 0:112, margin:, :]\n",
    "    volume[0:112, 112:, 0:76, :] = patches[2, 0:112, margin:, 0:76, :]\n",
    "    volume[0:112, 112:, 76:, :] = patches[3, 0:112, margin:, margin:, :]\n",
    "    volume[112:, 0:112, 0:76, :] = patches[4, margin:, 0:112, 0:76, :]\n",
    "    volume[112:, 0:112, 76:, :] = patches[5, margin:, 0:112, margin:, :]\n",
    "    volume[112:, 112:, 0:76, :] = patches[6, margin:, margin:, 0:76, :]\n",
    "    volume[112:, 112:, 76:, :] = patches[7, margin:, margin:, margin:, :]\n",
    "    return volume\n",
    "\n",
    "\n",
    "def aggregate2(patches):\n",
    "    margin = 8\n",
    "    volume = np.empty([112, 112, 76, 1])\n",
    "    volume[0:56, 0:56, 0:38, :] = patches[0, 0:56, 0:56, 0:38, :]\n",
    "    volume[0:56, 0:56, 38:, :] = patches[1, 0:56, 0:56, margin:, :]\n",
    "    volume[0:56, 56:, 0:38, :] = patches[2, 0:56, margin:, 0:38, :]\n",
    "    volume[0:56, 56:, 38:, :] = patches[3, 0:56, margin:, margin:, :]\n",
    "    volume[56:, 0:56, 0:38, :] = patches[4, margin:, 0:56, 0:38, :]\n",
    "    volume[56:, 0:56, 38:, :] = patches[5, margin:, 0:56, margin:, :]\n",
    "    volume[56:, 56:, 0:38, :] = patches[6, margin:, margin:, 0:38, :]\n",
    "    volume[56:, 56:, 38:, :] = patches[7, margin:, margin:, margin:, :]\n",
    "    return volume"
   ]
  },
  {
   "source": [
    "# Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import math\n",
    "import os\n",
    "from skimage.util import view_as_windows\n",
    "import glob  # For populating the list of files\n",
    "\n",
    "\n",
    "class Train_dataset(object):\n",
    "    def __init__(self, batch_size, overlapping=1):\n",
    "        self.batch_size = batch_size\n",
    "        # self.data_path = '/imatge/isanchez/projects/neuro/ADNI-Screening-1.5T'\n",
    "        self.data_path = \"C:/Users/Andres/Documents/data_prueba\"\n",
    "        # self.data_path = \"D:/Datasets/10.12751_g-node.aa605a/MALPEM_cross-sectional_seg138_5074\"\n",
    "        self.subject_list = os.listdir(self.data_path)\n",
    "        # self.subject_list = np.delete(self.subject_list, 120)\n",
    "        self.heigth_patch = 112  # 128\n",
    "        self.width_patch = 112  # 128\n",
    "        self.depth_patch = 76  # 92\n",
    "        self.margin = 16\n",
    "        self.overlapping = overlapping\n",
    "        self.num_patches = (math.ceil((224 / (self.heigth_patch)) / (self.overlapping))) * (\n",
    "            math.ceil((224 / (self.width_patch)) / (self.overlapping))) * (\n",
    "                               math.ceil((152 / (self.depth_patch)) / (self.overlapping)))\n",
    "\n",
    "    def mask(self, iteration):\n",
    "        subject_batch = self.subject_list[iteration * self.batch_size:self.batch_size + (iteration * self.batch_size)]\n",
    "        # subjects_true = np.empty([self.batch_size, 256, 256, 184])\n",
    "        subjects_true = np.empty([self.batch_size, 184, 256, 256])        \n",
    "        i = 0\n",
    "\n",
    "        path = \"D:/Datasets/ADNI/MASK/*\"\n",
    "\n",
    "        files = glob.glob(path)[:1]\n",
    "\n",
    "        for subject, file_name in zip(subject_batch, files):\n",
    "            if subject != 'ADNI_SCREENING_CLINICAL_FILE_08_02_17.csv':\n",
    "                # filename = os.path.join(self.data_path, subject)\n",
    "                # filename = os.path.join(filename, 'T1_brain_extractedBrainExtractionMask.nii.gz')\n",
    "                # filename = os.path.join(filename, file_name)\n",
    "                proxy = nib.load(file_name)\n",
    "                data = np.array(proxy.dataobj)\n",
    "\n",
    "                paddwidthr = int((256 - proxy.shape[2]) / 2)\n",
    "                paddheightr = int((256 - proxy.shape[1]) / 2)\n",
    "                paddepthr = int((184 - proxy.shape[0]) / 2)\n",
    "\n",
    "                if (paddwidthr * 2 + proxy.shape[2]) != 256:\n",
    "                    paddwidthl = paddwidthr + 1\n",
    "                else:\n",
    "                    paddwidthl = paddwidthr\n",
    "\n",
    "                if (paddheightr * 2 + proxy.shape[1]) != 256:\n",
    "                    paddheightl = paddheightr + 1\n",
    "                else:\n",
    "                    paddheightl = paddheightr\n",
    "\n",
    "                if (paddepthr * 2 + proxy.shape[0]) != 184:\n",
    "                    paddepthl = paddepthr + 1\n",
    "                else:\n",
    "                    paddepthl = paddepthr\n",
    "\n",
    "                data_padded = data\n",
    "\n",
    "                if paddwidthr + paddheightr + paddepthr != 0:\n",
    "                    data_padded = np.pad(data,\n",
    "                                        [(paddepthl, paddepthr), (paddheightl, paddheightr), (paddwidthl, paddwidthr)],\n",
    "                                        'constant', constant_values=0)\n",
    "\n",
    "                subjects_true[i] = data_padded[..., 0]\n",
    "                i = i + 1\n",
    "        mask = np.empty(\n",
    "            [self.batch_size * self.num_patches, self.width_patch + self.margin, self.heigth_patch + self.margin,\n",
    "             self.depth_patch + self.margin, 1])\n",
    "        i = 0\n",
    "        for subject in subjects_true:\n",
    "            patch = view_as_windows(subject, window_shape=(\n",
    "                (self.width_patch + self.margin), (self.heigth_patch + self.margin), (self.depth_patch + self.margin)),\n",
    "                                    step=(self.width_patch - self.margin, self.heigth_patch - self.margin,\n",
    "                                          self.depth_patch - self.margin))\n",
    "            for d in range(patch.shape[0]):\n",
    "                for v in range(patch.shape[1]):\n",
    "                    for h in range(patch.shape[2]):\n",
    "                        p = patch[d, v, h, :]\n",
    "                        p = p[:, np.newaxis]\n",
    "                        p = p.transpose((0, 2, 3, 1))\n",
    "                        mask[i] = p\n",
    "                        i = i + 1\n",
    "        return mask\n",
    "\n",
    "    def patches_true(self, iteration):\n",
    "        subjects_true = self.data_true(iteration)\n",
    "        patches_true = np.empty(\n",
    "            [self.batch_size * self.num_patches, self.width_patch + self.margin, self.heigth_patch + self.margin,\n",
    "             self.depth_patch + self.margin, 1])\n",
    "        i = 0\n",
    "        for subject in subjects_true:\n",
    "            patch = view_as_windows(subject, window_shape=(\n",
    "                (self.width_patch + self.margin), (self.heigth_patch + self.margin), (self.depth_patch + self.margin)),\n",
    "                                    step=(self.width_patch - self.margin, self.heigth_patch - self.margin,\n",
    "                                          self.depth_patch - self.margin))\n",
    "            for d in range(patch.shape[0]):\n",
    "                for v in range(patch.shape[1]):\n",
    "                    for h in range(patch.shape[2]):\n",
    "                        p = patch[d, v, h, :]\n",
    "                        p = p[:, np.newaxis]\n",
    "                        p = p.transpose((0, 2, 3, 1))\n",
    "                        patches_true[i] = p\n",
    "                        i = i + 1\n",
    "        return patches_true\n",
    "\n",
    "    def data_true(self, iteration):\n",
    "        subject_batch = self.subject_list[iteration * self.batch_size:self.batch_size + (iteration * self.batch_size)]\n",
    "        # subjects = np.empty([self.batch_size, 224, 224, 152])\n",
    "        subjects = np.empty([self.batch_size, 168, 224, 152])\n",
    "        # files = os.listdir(self.data_path)\n",
    "\n",
    "        # path = \"D:/Datasets/ADNI/*/*/*/*/*\"\n",
    "        path = \"D:/Datasets/ADNI/MRI/*\"\n",
    "        files = glob.glob(path)[:1]\n",
    "\n",
    "        i = 0\n",
    "        for subject, file_name in zip(subject_batch, files):\n",
    "            if subject != 'ADNI_SCREENING_CLINICAL_FILE_08_02_17.csv':\n",
    "                # filename = os.path.join(self.data_path, subject)\n",
    "                # filename = os.path.join(filename, 'T1_brain_extractedBrainExtractionBrain.nii.gz')\n",
    "\n",
    "                # filename = os.path.join(self.data_path, file_name)\n",
    "\n",
    "                proxy = nib.load(file_name)\n",
    "                data = np.array(proxy.dataobj)\n",
    "\n",
    "\n",
    "                paddwidthr = int((256 - proxy.shape[2]) / 2)\n",
    "                paddheightr = int((256 - proxy.shape[1]) / 2)\n",
    "                paddepthr = int((184 - proxy.shape[0]) / 2)\n",
    "\n",
    "                if (paddwidthr * 2 + proxy.shape[2]) != 256:\n",
    "                    paddwidthl = paddwidthr + 1\n",
    "                else:\n",
    "                    paddwidthl = paddwidthr\n",
    "\n",
    "                if (paddheightr * 2 + proxy.shape[1]) != 256:\n",
    "                    paddheightl = paddheightr + 1\n",
    "                else:\n",
    "                    paddheightl = paddheightr\n",
    "\n",
    "                if (paddepthr * 2 + proxy.shape[0]) != 184:\n",
    "                    paddepthl = paddepthr + 1\n",
    "                else:\n",
    "                    paddepthl = paddepthr\n",
    "                \n",
    "                data_padded = data\n",
    "\n",
    "                if paddwidthr + paddheightr + paddepthr != 0:\n",
    "                    data_padded = np.pad(data,\n",
    "                                        [(paddepthl, paddepthr), (paddheightl, paddheightr), (paddwidthl, paddwidthr)],\n",
    "                                        'constant', constant_values=0)\n",
    "                # data_padded = np.pad(data,\n",
    "                #                      [(paddwidthl, paddwidthr), (paddheightl, paddheightr), (paddepthl, paddepthr)],\n",
    "                #                      'constant', constant_values=0)\n",
    "\n",
    "                subjects[i] = data_padded[16:240, 16:240, 16:168, 0]  # remove background\n",
    "                i = i + 1\n",
    "        return subjects"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dLayer SRGAN_g/conv1: shape: [3, 3, 3, 1, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN-conv1: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/0: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/0: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/0: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/0: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/0: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/1: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/1: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/1: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/1: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/1: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/2: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/2: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/3: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/3: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/3: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/3: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/3: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/4: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/4: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/4: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/4: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/4: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/5: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/5: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/5: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/5: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/5: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/6: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/6: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/6: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/6: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/6: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/7: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/7: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/7: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/7: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/7: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN-conv2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-conv2: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] DeConv3dLayer SRGAN_g/conv1-ub-subpixelnn/1: shape: [6, 6, 6, 64, 64] out_shape: [<tf.Tensor 'SRGAN_g/strided_slice:0' shape=() dtype=int32>, 128, 128, 92, 64] strides: [1, 2, 2, 2, 1] pad: SAME act: lrelu1\n",
      "[TL] Conv3dLayer SRGAN_g/convlast-subpixelnn: shape: [3, 3, 3, 64, 1] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] InputLayer  SRGAN_d/in: (2, 128, 128, 92, 1)\n",
      "[TL] Conv3dLayer SRGAN_d/conv1: shape: [3, 3, 3, 1, 32] strides: [1, 1, 1, 1, 1] pad: SAME act: lrelu2\n",
      "[TL] Conv3dLayer SRGAN_d/conv2: shape: [3, 3, 3, 32, 32] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv2: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv3: shape: [3, 3, 3, 32, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv3: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv4: shape: [3, 3, 3, 64, 64] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv4: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv5: shape: [3, 3, 3, 64, 128] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv5: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv6: shape: [3, 3, 3, 128, 128] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv6: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv7: shape: [3, 3, 3, 128, 256] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv7: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv8: shape: [3, 3, 3, 256, 256] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv8: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] FlattenLayer SRGAN_d/flatten: 98304\n",
      "[TL] DenseLayer  SRGAN_d/dense1: 1024 lrelu2\n",
      "[TL] DenseLayer  SRGAN_d/dense2: 1 No Activation\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] InputLayer  SRGAN_d/in: (2, 128, 128, 92, 1)\n",
      "[TL] Conv3dLayer SRGAN_d/conv1: shape: [3, 3, 3, 1, 32] strides: [1, 1, 1, 1, 1] pad: SAME act: lrelu2\n",
      "[TL] Conv3dLayer SRGAN_d/conv2: shape: [3, 3, 3, 32, 32] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv2: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv3: shape: [3, 3, 3, 32, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv3: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv4: shape: [3, 3, 3, 64, 64] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv4: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv5: shape: [3, 3, 3, 64, 128] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv5: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv6: shape: [3, 3, 3, 128, 128] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv6: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv7: shape: [3, 3, 3, 128, 256] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv7: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_d/conv8: shape: [3, 3, 3, 256, 256] strides: [1, 2, 2, 2, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_d/BN1-conv8: decay: 0.900000 epsilon: 0.000010 act: lrelu2 is_train: True\n",
      "[TL] FlattenLayer SRGAN_d/flatten: 98304\n",
      "[TL] DenseLayer  SRGAN_d/dense1: 1024 lrelu2\n",
      "[TL] DenseLayer  SRGAN_d/dense2: 1 No Activation\n",
      "[TL] WARNING: this method is DEPRECATED and has no effect, please remove it from your code.\n",
      "[TL] InputLayer  SRGAN_g/in: (2, ?, ?, ?, 1)\n",
      "[TL] Conv3dLayer SRGAN_g/conv1: shape: [3, 3, 3, 1, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN-conv1: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/0: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/0: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/0: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/0: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/0: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/1: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/1: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/1: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/1: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/1: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/2: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/2: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/3: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/3: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/3: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/3: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/3: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/4: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/4: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/4: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/4: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/4: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/5: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/5: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/5: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/5: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/5: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/6: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/6: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/6: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/6: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/6: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv1-rb/7: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN1-rb/7: decay: 0.900000 epsilon: 0.000010 act: lrelu1 is_train: True\n",
      "[TL] Conv3dLayer SRGAN_g/conv2-rb/7: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN2-rb/7: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-rb/7: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] Conv3dLayer SRGAN_g/conv2: shape: [3, 3, 3, 64, 64] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL] BatchNormLayer SRGAN_g/BN-conv2: decay: 0.900000 epsilon: 0.000010 act: No Activation is_train: True\n",
      "[TL] ElementwiseLayer SRGAN_g/add-conv2: size: (2, ?, ?, ?, 64) fn: add\n",
      "[TL] DeConv3dLayer SRGAN_g/conv1-ub-subpixelnn/1: shape: [6, 6, 6, 64, 64] out_shape: [<tf.Tensor 'SRGAN_g_1/strided_slice:0' shape=() dtype=int32>, 128, 128, 92, 64] strides: [1, 2, 2, 2, 1] pad: SAME act: lrelu1\n",
      "[TL] Conv3dLayer SRGAN_g/convlast-subpixelnn: shape: [3, 3, 3, 64, 1] strides: [1, 1, 1, 1, 1] pad: SAME act: No Activation\n",
      "[TL]   [*] geting variables with SRGAN_g\n",
      "[TL]   got   0: SRGAN_g/conv1/W_conv3d:0   (3, 3, 3, 1, 64)\n",
      "[TL]   got   1: SRGAN_g/conv1/b_conv3d:0   (64,)\n",
      "[TL]   got   2: SRGAN_g/BN-conv1/beta:0   (64,)\n",
      "[TL]   got   3: SRGAN_g/BN-conv1/gamma:0   (64,)\n",
      "[TL]   got   4: SRGAN_g/conv1-rb/0/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got   5: SRGAN_g/conv1-rb/0/b_conv3d:0   (64,)\n",
      "[TL]   got   6: SRGAN_g/BN1-rb/0/beta:0   (64,)\n",
      "[TL]   got   7: SRGAN_g/BN1-rb/0/gamma:0   (64,)\n",
      "[TL]   got   8: SRGAN_g/conv2-rb/0/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got   9: SRGAN_g/conv2-rb/0/b_conv3d:0   (64,)\n",
      "[TL]   got  10: SRGAN_g/BN2-rb/0/beta:0   (64,)\n",
      "[TL]   got  11: SRGAN_g/BN2-rb/0/gamma:0   (64,)\n",
      "[TL]   got  12: SRGAN_g/conv1-rb/1/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  13: SRGAN_g/conv1-rb/1/b_conv3d:0   (64,)\n",
      "[TL]   got  14: SRGAN_g/BN1-rb/1/beta:0   (64,)\n",
      "[TL]   got  15: SRGAN_g/BN1-rb/1/gamma:0   (64,)\n",
      "[TL]   got  16: SRGAN_g/conv2-rb/1/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  17: SRGAN_g/conv2-rb/1/b_conv3d:0   (64,)\n",
      "[TL]   got  18: SRGAN_g/BN2-rb/1/beta:0   (64,)\n",
      "[TL]   got  19: SRGAN_g/BN2-rb/1/gamma:0   (64,)\n",
      "[TL]   got  20: SRGAN_g/conv1-rb/2/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  21: SRGAN_g/conv1-rb/2/b_conv3d:0   (64,)\n",
      "[TL]   got  22: SRGAN_g/BN1-rb/2/beta:0   (64,)\n",
      "[TL]   got  23: SRGAN_g/BN1-rb/2/gamma:0   (64,)\n",
      "[TL]   got  24: SRGAN_g/conv2-rb/2/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  25: SRGAN_g/conv2-rb/2/b_conv3d:0   (64,)\n",
      "[TL]   got  26: SRGAN_g/BN2-rb/2/beta:0   (64,)\n",
      "[TL]   got  27: SRGAN_g/BN2-rb/2/gamma:0   (64,)\n",
      "[TL]   got  28: SRGAN_g/conv1-rb/3/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  29: SRGAN_g/conv1-rb/3/b_conv3d:0   (64,)\n",
      "[TL]   got  30: SRGAN_g/BN1-rb/3/beta:0   (64,)\n",
      "[TL]   got  31: SRGAN_g/BN1-rb/3/gamma:0   (64,)\n",
      "[TL]   got  32: SRGAN_g/conv2-rb/3/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  33: SRGAN_g/conv2-rb/3/b_conv3d:0   (64,)\n",
      "[TL]   got  34: SRGAN_g/BN2-rb/3/beta:0   (64,)\n",
      "[TL]   got  35: SRGAN_g/BN2-rb/3/gamma:0   (64,)\n",
      "[TL]   got  36: SRGAN_g/conv1-rb/4/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  37: SRGAN_g/conv1-rb/4/b_conv3d:0   (64,)\n",
      "[TL]   got  38: SRGAN_g/BN1-rb/4/beta:0   (64,)\n",
      "[TL]   got  39: SRGAN_g/BN1-rb/4/gamma:0   (64,)\n",
      "[TL]   got  40: SRGAN_g/conv2-rb/4/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  41: SRGAN_g/conv2-rb/4/b_conv3d:0   (64,)\n",
      "[TL]   got  42: SRGAN_g/BN2-rb/4/beta:0   (64,)\n",
      "[TL]   got  43: SRGAN_g/BN2-rb/4/gamma:0   (64,)\n",
      "[TL]   got  44: SRGAN_g/conv1-rb/5/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  45: SRGAN_g/conv1-rb/5/b_conv3d:0   (64,)\n",
      "[TL]   got  46: SRGAN_g/BN1-rb/5/beta:0   (64,)\n",
      "[TL]   got  47: SRGAN_g/BN1-rb/5/gamma:0   (64,)\n",
      "[TL]   got  48: SRGAN_g/conv2-rb/5/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  49: SRGAN_g/conv2-rb/5/b_conv3d:0   (64,)\n",
      "[TL]   got  50: SRGAN_g/BN2-rb/5/beta:0   (64,)\n",
      "[TL]   got  51: SRGAN_g/BN2-rb/5/gamma:0   (64,)\n",
      "[TL]   got  52: SRGAN_g/conv1-rb/6/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  53: SRGAN_g/conv1-rb/6/b_conv3d:0   (64,)\n",
      "[TL]   got  54: SRGAN_g/BN1-rb/6/beta:0   (64,)\n",
      "[TL]   got  55: SRGAN_g/BN1-rb/6/gamma:0   (64,)\n",
      "[TL]   got  56: SRGAN_g/conv2-rb/6/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  57: SRGAN_g/conv2-rb/6/b_conv3d:0   (64,)\n",
      "[TL]   got  58: SRGAN_g/BN2-rb/6/beta:0   (64,)\n",
      "[TL]   got  59: SRGAN_g/BN2-rb/6/gamma:0   (64,)\n",
      "[TL]   got  60: SRGAN_g/conv1-rb/7/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  61: SRGAN_g/conv1-rb/7/b_conv3d:0   (64,)\n",
      "[TL]   got  62: SRGAN_g/BN1-rb/7/beta:0   (64,)\n",
      "[TL]   got  63: SRGAN_g/BN1-rb/7/gamma:0   (64,)\n",
      "[TL]   got  64: SRGAN_g/conv2-rb/7/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  65: SRGAN_g/conv2-rb/7/b_conv3d:0   (64,)\n",
      "[TL]   got  66: SRGAN_g/BN2-rb/7/beta:0   (64,)\n",
      "[TL]   got  67: SRGAN_g/BN2-rb/7/gamma:0   (64,)\n",
      "[TL]   got  68: SRGAN_g/conv2/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  69: SRGAN_g/conv2/b_conv3d:0   (64,)\n",
      "[TL]   got  70: SRGAN_g/BN-conv2/beta:0   (64,)\n",
      "[TL]   got  71: SRGAN_g/BN-conv2/gamma:0   (64,)\n",
      "[TL]   got  72: SRGAN_g/conv1-ub-subpixelnn/1/W_deconv3d:0   (6, 6, 6, 64, 64)\n",
      "[TL]   got  73: SRGAN_g/conv1-ub-subpixelnn/1/b_deconv3d:0   (64,)\n",
      "[TL]   got  74: SRGAN_g/convlast-subpixelnn/W_conv3d:0   (3, 3, 3, 64, 1)\n",
      "[TL]   got  75: SRGAN_g/convlast-subpixelnn/b_conv3d:0   (1,)\n",
      "[TL]   [*] geting variables with SRGAN_d\n",
      "[TL]   got   0: SRGAN_d/conv1/W_conv3d:0   (3, 3, 3, 1, 32)\n",
      "[TL]   got   1: SRGAN_d/conv1/b_conv3d:0   (32,)\n",
      "[TL]   got   2: SRGAN_d/conv2/W_conv3d:0   (3, 3, 3, 32, 32)\n",
      "[TL]   got   3: SRGAN_d/conv2/b_conv3d:0   (32,)\n",
      "[TL]   got   4: SRGAN_d/BN1-conv2/beta:0   (32,)\n",
      "[TL]   got   5: SRGAN_d/BN1-conv2/gamma:0   (32,)\n",
      "[TL]   got   6: SRGAN_d/conv3/W_conv3d:0   (3, 3, 3, 32, 64)\n",
      "[TL]   got   7: SRGAN_d/conv3/b_conv3d:0   (64,)\n",
      "[TL]   got   8: SRGAN_d/BN1-conv3/beta:0   (64,)\n",
      "[TL]   got   9: SRGAN_d/BN1-conv3/gamma:0   (64,)\n",
      "[TL]   got  10: SRGAN_d/conv4/W_conv3d:0   (3, 3, 3, 64, 64)\n",
      "[TL]   got  11: SRGAN_d/conv4/b_conv3d:0   (64,)\n",
      "[TL]   got  12: SRGAN_d/BN1-conv4/beta:0   (64,)\n",
      "[TL]   got  13: SRGAN_d/BN1-conv4/gamma:0   (64,)\n",
      "[TL]   got  14: SRGAN_d/conv5/W_conv3d:0   (3, 3, 3, 64, 128)\n",
      "[TL]   got  15: SRGAN_d/conv5/b_conv3d:0   (128,)\n",
      "[TL]   got  16: SRGAN_d/BN1-conv5/beta:0   (128,)\n",
      "[TL]   got  17: SRGAN_d/BN1-conv5/gamma:0   (128,)\n",
      "[TL]   got  18: SRGAN_d/conv6/W_conv3d:0   (3, 3, 3, 128, 128)\n",
      "[TL]   got  19: SRGAN_d/conv6/b_conv3d:0   (128,)\n",
      "[TL]   got  20: SRGAN_d/BN1-conv6/beta:0   (128,)\n",
      "[TL]   got  21: SRGAN_d/BN1-conv6/gamma:0   (128,)\n",
      "[TL]   got  22: SRGAN_d/conv7/W_conv3d:0   (3, 3, 3, 128, 256)\n",
      "[TL]   got  23: SRGAN_d/conv7/b_conv3d:0   (256,)\n",
      "[TL]   got  24: SRGAN_d/BN1-conv7/beta:0   (256,)\n",
      "[TL]   got  25: SRGAN_d/BN1-conv7/gamma:0   (256,)\n",
      "[TL]   got  26: SRGAN_d/conv8/W_conv3d:0   (3, 3, 3, 256, 256)\n",
      "[TL]   got  27: SRGAN_d/conv8/b_conv3d:0   (256,)\n",
      "[TL]   got  28: SRGAN_d/BN1-conv8/beta:0   (256,)\n",
      "[TL]   got  29: SRGAN_d/BN1-conv8/gamma:0   (256,)\n",
      "[TL]   got  30: SRGAN_d/dense1/W:0   (98304, 1024)\n",
      "[TL]   got  31: SRGAN_d/dense1/b:0   (1024,)\n",
      "[TL]   got  32: SRGAN_d/dense2/W:0   (1024, 1)\n",
      "[TL]   got  33: SRGAN_d/dense2/b:0   (1,)\n",
      "[TL] WARNING: Function: `tensorlayer.layers.utils.initialize_global_variables` (in file: C:\\Users\\Andres\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorlayer\\layers\\utils.py) is deprecated and will be removed after 2018-09-30.\n",
      "Instructions for updating: This API is deprecated in favor of `tf.global_variables_initializer`\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-191a05db5d87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    515\u001b[0m               \u001b[0msubpixel_NN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubpixel_NN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresidual_blocks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresidual_blocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m               \u001b[0mpath_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath_prediction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 517\u001b[1;33m               img_height=128, img_depth=92, batch_size=1, restore=restore)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-191a05db5d87>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(upscaling_factor, residual_blocks, feature_size, path_prediction, checkpoint_dir, img_width, img_height, img_depth, subpixel_NN, nn, restore, batch_size, div_patches, epochs)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[0mtl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize_global_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\wrapt\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m         return self._self_wrapper(self.__wrapped__, self._self_instance,\n\u001b[1;32m--> 523\u001b[1;33m                 args, kwargs)\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBoundFunctionWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_FunctionWrapperBase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorlayer\\decorators\\deprecated.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m                 )\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mdecorated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorlayer\\layers\\utils.py\u001b[0m in \u001b[0;36minitialize_global_variables\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'The session must be defined'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\gan-3d-s\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "from tensorlayer.layers import *\n",
    "import math\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "import nibabel as nib\n",
    "import os\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "from skimage.measure import compare_psnr as psnr\n",
    "from keras.layers.convolutional import UpSampling3D\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "from dataset import Train_dataset\n",
    "from utils import smooth_gan_labels, aggregate, subPixelConv3d\n",
    "\n",
    "def lrelu1(x):\n",
    "    return tf.maximum(x, 0.25 * x)\n",
    "\n",
    "\n",
    "def lrelu2(x):\n",
    "    return tf.maximum(x, 0.3 * x)\n",
    "\n",
    "\n",
    "def discriminator(input_disc, kernel, reuse, is_train=True):\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "    batch_size = 1\n",
    "    div_patches = 4\n",
    "    num_patches = 8\n",
    "    img_width = 128\n",
    "    img_height = 128\n",
    "    img_depth = 92\n",
    "    with tf.variable_scope(\"SRGAN_d\", reuse=reuse):\n",
    "        tl.layers.set_name_reuse(reuse)\n",
    "        input_disc.set_shape([int((batch_size * num_patches) / div_patches), img_width, img_height, img_depth, 1], )\n",
    "        x = InputLayer(input_disc, name='in')\n",
    "        x = Conv3dLayer(x, act=lrelu2, shape=[kernel, kernel, kernel, 1, 32], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv1')\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 32, 32], strides=[1, 2, 2, 2, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv2')\n",
    "\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv2', act=lrelu2)\n",
    "\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 32, 64], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv3')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv3', act=lrelu2)\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 64, 64], strides=[1, 2, 2, 2, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv4')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv4', act=lrelu2)\n",
    "\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 64, 128], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv5')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv5', act=lrelu2)\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 128, 128], strides=[1, 2, 2, 2, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv6')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv6', act=lrelu2)\n",
    "\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 128, 256], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv7')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv7', act=lrelu2)\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 256, 256], strides=[1, 2, 2, 2, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv8')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN1-conv8', act=lrelu2)\n",
    "\n",
    "        x = FlattenLayer(x, name='flatten')\n",
    "        x = DenseLayer(x, n_units=1024, act=lrelu2, name='dense1')\n",
    "        x = DenseLayer(x, n_units=1, name='dense2')\n",
    "\n",
    "        logits = x.outputs\n",
    "        x.outputs = tf.nn.sigmoid(x.outputs, name='output')\n",
    "\n",
    "        return x, logits\n",
    "\n",
    "\n",
    "def generator(input_gen, kernel, nb, upscaling_factor, reuse, feature_size, img_width, img_height, img_depth,\n",
    "              subpixel_NN, nn, is_train=True):\n",
    "    w_init = tf.random_normal_initializer(stddev=0.02)\n",
    "\n",
    "    w_init_subpixel1 = np.random.normal(scale=0.02, size=[3, 3, 3, 64, feature_size])\n",
    "    w_init_subpixel1 = zoom(w_init_subpixel1, [2, 2, 2, 1, 1], order=0)\n",
    "    w_init_subpixel1_last = tf.constant_initializer(w_init_subpixel1)\n",
    "    w_init_subpixel2 = np.random.normal(scale=0.02, size=[3, 3, 3, 64, 64])\n",
    "    w_init_subpixel2 = zoom(w_init_subpixel2, [2, 2, 2, 1, 1], order=0)\n",
    "    w_init_subpixel2_last = tf.constant_initializer(w_init_subpixel2)\n",
    "\n",
    "    with tf.variable_scope(\"SRGAN_g\", reuse=reuse):\n",
    "        tl.layers.set_name_reuse(reuse)\n",
    "        x = InputLayer(input_gen, name='in')\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 1, feature_size], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv1')\n",
    "        x = BatchNormLayer(x, act=lrelu1, is_train=is_train, name='BN-conv1')\n",
    "        inputRB = x\n",
    "        inputadd = x\n",
    "\n",
    "        # residual blocks\n",
    "        for i in range(nb):\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, feature_size, feature_size], strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='conv1-rb/%s' % i)\n",
    "            x = BatchNormLayer(x, act=lrelu1, is_train=is_train, name='BN1-rb/%s' % i)\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, feature_size, feature_size], strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='conv2-rb/%s' % i)\n",
    "            x = BatchNormLayer(x, is_train=is_train, name='BN2-rb/%s' % i)\n",
    "            # short skip connection\n",
    "            x = ElementwiseLayer([x, inputadd], tf.add, name='add-rb/%s' % i)\n",
    "            inputadd = x\n",
    "\n",
    "        # large skip connection\n",
    "        x = Conv3dLayer(x, shape=[kernel, kernel, kernel, feature_size, feature_size], strides=[1, 1, 1, 1, 1],\n",
    "                        padding='SAME', W_init=w_init, name='conv2')\n",
    "        x = BatchNormLayer(x, is_train=is_train, name='BN-conv2')\n",
    "        x = ElementwiseLayer([x, inputRB], tf.add, name='add-conv2')\n",
    "\n",
    "        # ____________SUBPIXEL-NN______________#\n",
    "\n",
    "        if subpixel_NN:\n",
    "            # upscaling block 1\n",
    "            if upscaling_factor == 4:\n",
    "                img_height_deconv = int(img_height / 2)\n",
    "                img_width_deconv = int(img_width / 2)\n",
    "                img_depth_deconv = int(img_depth / 2)\n",
    "            else:\n",
    "                img_height_deconv = img_height\n",
    "                img_width_deconv = img_width\n",
    "                img_depth_deconv = img_depth\n",
    "\n",
    "            x = DeConv3dLayer(x, shape=[kernel * 2, kernel * 2, kernel * 2, 64, feature_size],\n",
    "                              act=lrelu1, strides=[1, 2, 2, 2, 1],\n",
    "                              output_shape=[tf.shape(input_gen)[0], img_height_deconv, img_width_deconv,\n",
    "                                            img_depth_deconv, 64],\n",
    "                              padding='SAME', W_init=w_init_subpixel1_last, name='conv1-ub-subpixelnn/1')\n",
    "\n",
    "            # upscaling block 2\n",
    "            if upscaling_factor == 4:\n",
    "                x = DeConv3dLayer(x, shape=[kernel * 2, kernel * 2, kernel * 2, 64, 64],\n",
    "                                  act=lrelu1, strides=[1, 2, 2, 2, 1], padding='SAME',\n",
    "                                  output_shape=[tf.shape(input_gen)[0], img_height, img_width,\n",
    "                                                img_depth, 64],\n",
    "                                  W_init=w_init_subpixel2_last, name='conv1-ub-subpixelnn/2')\n",
    "\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 64, 1], strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='convlast-subpixelnn')\n",
    "\n",
    "        # ____________RC______________#\n",
    "\n",
    "        elif nn:\n",
    "            # upscaling block 1\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, feature_size, 64], act=lrelu1,\n",
    "                            strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='conv1-ub/1')\n",
    "            x = UpSampling3D(name='UpSampling3D_1')(x.outputs)\n",
    "            x = Conv3dLayer(InputLayer(x, name='in ub1 conv2'),\n",
    "                            shape=[kernel, kernel, kernel, 64, 64],\n",
    "                            act=lrelu1,\n",
    "                            strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='conv2-ub/1')\n",
    "\n",
    "            # upscaling block 2\n",
    "            if upscaling_factor == 4:\n",
    "                x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 64, 64], act=lrelu1,\n",
    "                                strides=[1, 1, 1, 1, 1],\n",
    "                                padding='SAME', W_init=w_init, name='conv1-ub/2')\n",
    "                x = UpSampling3D(name='UpSampling3D_1')(x.outputs)\n",
    "                x = Conv3dLayer(InputLayer(x, name='in ub2 conv2'), shape=[kernel, kernel, kernel, 64,\n",
    "                                                                           64], act=lrelu1,\n",
    "                                strides=[1, 1, 1, 1, 1],\n",
    "                                padding='SAME', W_init=w_init, name='conv2-ub/2')\n",
    "\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, 64, 1], strides=[1, 1, 1, 1, 1],\n",
    "                            act=tf.nn.tanh, padding='SAME', W_init=w_init, name='convlast')\n",
    "\n",
    "        # ____________SUBPIXEL - BASELINE______________#\n",
    "\n",
    "        else:\n",
    "\n",
    "            if upscaling_factor == 4:\n",
    "                steps_to_end = 2\n",
    "            else:\n",
    "                steps_to_end = 1\n",
    "\n",
    "            # upscaling block 1\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, feature_size, 64], act=lrelu1,\n",
    "                            strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='conv1-ub/1')\n",
    "            arguments = {'img_width': img_width, 'img_height': img_height, 'img_depth': img_depth,\n",
    "                         'stepsToEnd': steps_to_end,\n",
    "                         'n_out_channel': int(64 / 8)}\n",
    "            x = LambdaLayer(x, fn=subPixelConv3d, fn_args=arguments, name='SubPixel1')\n",
    "\n",
    "            # upscaling block 2\n",
    "            if upscaling_factor == 4:\n",
    "                x = Conv3dLayer(x, shape=[kernel, kernel, kernel, int((64) / 8), 64], act=lrelu1,\n",
    "                                strides=[1, 1, 1, 1, 1],\n",
    "                                padding='SAME', W_init=w_init, name='conv1-ub/2')\n",
    "                arguments = {'img_width': img_width, 'img_height': img_height, 'img_depth': img_depth, 'stepsToEnd': 1,\n",
    "                             'n_out_channel': int(64 / 8)}\n",
    "                x = LambdaLayer(x, fn=subPixelConv3d, fn_args=arguments, name='SubPixel2')\n",
    "\n",
    "            x = Conv3dLayer(x, shape=[kernel, kernel, kernel, int(64 / 8), 1], strides=[1, 1, 1, 1, 1],\n",
    "                            padding='SAME', W_init=w_init, name='convlast')\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(upscaling_factor, residual_blocks, feature_size, path_prediction, checkpoint_dir, img_width, img_height,\n",
    "          img_depth, subpixel_NN, nn, restore, batch_size=1, div_patches=4, epochs=10):\n",
    "    traindataset = Train_dataset(batch_size)\n",
    "    iterations_train = math.ceil((len(traindataset.subject_list) * 0.8) / batch_size)\n",
    "    num_patches = traindataset.num_patches\n",
    "\n",
    "    # ##========================== DEFINE MODEL ============================##\n",
    "    t_input_gen = tf.placeholder('float32', [int((batch_size * num_patches) / div_patches), None,\n",
    "                                             None, None, 1],\n",
    "                                 name='t_image_input_to_SRGAN_generator')\n",
    "    t_target_image = tf.placeholder('float32', [int((batch_size * num_patches) / div_patches),\n",
    "                                                img_width, img_height, img_depth, 1],\n",
    "                                    name='t_target_image')\n",
    "    t_input_mask = tf.placeholder('float32', [int((batch_size * num_patches) / div_patches),\n",
    "                                              img_width, img_height, img_depth, 1],\n",
    "                                  name='t_image_input_mask')\n",
    "\n",
    "    net_gen = generator(input_gen=t_input_gen, kernel=3, nb=residual_blocks, upscaling_factor=upscaling_factor,\n",
    "                        img_height=img_height, img_width=img_width, img_depth=img_depth, subpixel_NN=subpixel_NN, nn=nn,\n",
    "                        feature_size=feature_size, is_train=True, reuse=False)\n",
    "\n",
    "    net_d, disc_out_real = discriminator(input_disc=t_target_image, kernel=3, is_train=True, reuse=False)\n",
    "    _, disc_out_fake = discriminator(input_disc=net_gen.outputs, kernel=3, is_train=True, reuse=True)\n",
    "\n",
    "    # test\n",
    "    gen_test = generator(t_input_gen, kernel=3, nb=residual_blocks, upscaling_factor=upscaling_factor,\n",
    "                         img_height=img_height, img_width=img_width, img_depth=img_depth, subpixel_NN=subpixel_NN,\n",
    "                         nn=nn,\n",
    "                         feature_size=feature_size, is_train=True, reuse=True)\n",
    "\n",
    "    # ###========================== DEFINE TRAIN OPS ==========================###\n",
    "\n",
    "    if np.random.uniform() > 0.1:\n",
    "        # give correct classifications\n",
    "        y_gan_real = tf.ones_like(disc_out_real)\n",
    "        y_gan_fake = tf.zeros_like(disc_out_real)\n",
    "    else:\n",
    "        # give wrong classifications (noisy labels)\n",
    "        y_gan_real = tf.zeros_like(disc_out_real)\n",
    "        y_gan_fake = tf.ones_like(disc_out_real)\n",
    "\n",
    "    d_loss_real = tf.reduce_mean(tf.square(disc_out_real - smooth_gan_labels(y_gan_real)),\n",
    "                                 name='d_loss_real')\n",
    "    d_loss_fake = tf.reduce_mean(tf.square(disc_out_fake - smooth_gan_labels(y_gan_fake)),\n",
    "                                 name='d_loss_fake')\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    mse_loss = tf.reduce_sum(\n",
    "        tf.square(net_gen.outputs - t_target_image), axis=[0, 1, 2, 3, 4], name='g_loss_mse')\n",
    "\n",
    "    dx_real = t_target_image[:, 1:, :, :, :] - t_target_image[:, :-1, :, :, :]\n",
    "    dy_real = t_target_image[:, :, 1:, :, :] - t_target_image[:, :, :-1, :, :]\n",
    "    dz_real = t_target_image[:, :, :, 1:, :] - t_target_image[:, :, :, :-1, :]\n",
    "    dx_fake = net_gen.outputs[:, 1:, :, :, :] - net_gen.outputs[:, :-1, :, :, :]\n",
    "    dy_fake = net_gen.outputs[:, :, 1:, :, :] - net_gen.outputs[:, :, :-1, :, :]\n",
    "    dz_fake = net_gen.outputs[:, :, :, 1:, :] - net_gen.outputs[:, :, :, :-1, :]\n",
    "\n",
    "    gd_loss = tf.reduce_sum(tf.square(tf.abs(dx_real) - tf.abs(dx_fake))) + \\\n",
    "              tf.reduce_sum(tf.square(tf.abs(dy_real) - tf.abs(dy_fake))) + \\\n",
    "              tf.reduce_sum(tf.square(tf.abs(dz_real) - tf.abs(dz_fake)))\n",
    "\n",
    "    g_gan_loss = 10e-2 * tf.reduce_mean(tf.square(disc_out_fake - smooth_gan_labels(tf.ones_like(disc_out_real))),\n",
    "                                        name='g_loss_gan')\n",
    "\n",
    "    g_loss = mse_loss + g_gan_loss + gd_loss\n",
    "\n",
    "    g_vars = tl.layers.get_variables_with_name('SRGAN_g', True, True)\n",
    "    d_vars = tl.layers.get_variables_with_name('SRGAN_d', True, True)\n",
    "\n",
    "    with tf.variable_scope('learning_rate'):\n",
    "        lr_v = tf.Variable(1e-4, trainable=False)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    decay_rate = 0.5\n",
    "    decay_steps = 4920  # every 2 epochs (more or less)\n",
    "    learning_rate = tf.train.inverse_time_decay(lr_v, global_step=global_step, decay_rate=decay_rate,\n",
    "                                                decay_steps=decay_steps)\n",
    "\n",
    "    # Optimizers\n",
    "    g_optim = tf.train.AdamOptimizer(learning_rate).minimize(g_loss, var_list=g_vars)\n",
    "    d_optim = tf.train.AdamOptimizer(learning_rate).minimize(d_loss, var_list=d_vars)\n",
    "\n",
    "    session = tf.Session()\n",
    "    tl.layers.initialize_global_variables(session)\n",
    "\n",
    "    step = 0\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    if restore is not None:\n",
    "        saver.restore(session, tf.train.latest_checkpoint(restore))\n",
    "        val_restore = 0 * epochs\n",
    "    else:\n",
    "        val_restore = 0\n",
    "\n",
    "    array_psnr = []\n",
    "    array_ssim = []\n",
    "\n",
    "    for j in range(val_restore, epochs + val_restore):\n",
    "        for i in range(0, iterations_train):\n",
    "            # ====================== LOAD DATA =========================== #\n",
    "            xt_total = traindataset.patches_true(i)\n",
    "            xm_total = traindataset.mask(i)\n",
    "            for k in range(0, div_patches):\n",
    "                print('{}'.format(k))\n",
    "                xt = xt_total[k * int((batch_size * num_patches) / div_patches):(int(\n",
    "                    (batch_size * num_patches) / div_patches) * k) + int(\n",
    "                    (batch_size * num_patches) / div_patches)]\n",
    "                xm = xm_total[k * int((batch_size * num_patches) / div_patches):(int(\n",
    "                    (batch_size * num_patches) / div_patches) * k) + int(\n",
    "                    (batch_size * num_patches) / div_patches)]\n",
    "\n",
    "                # NORMALIZING\n",
    "                for t in range(0, xt.shape[0]):\n",
    "                    normfactor = (np.amax(xt[t])) / 2\n",
    "                    if normfactor != 0:\n",
    "                        xt[t] = ((xt[t] - normfactor) / normfactor)\n",
    "\n",
    "                x_generator = gaussian_filter(xt, sigma=1)\n",
    "                x_generator = zoom(x_generator, [1, (1 / upscaling_factor), (1 / upscaling_factor),\n",
    "                                                 (1 / upscaling_factor), 1], prefilter=False, order=0)\n",
    "                xgenin = x_generator\n",
    "\n",
    "                # ========================= train SRGAN ========================= #\n",
    "                # update D\n",
    "                errd, _ = session.run([d_loss, d_optim], {t_target_image: xt, t_input_gen: xgenin})\n",
    "                # update G\n",
    "                errg, errmse, errgan, errgd, _ = session.run([g_loss, mse_loss, g_gan_loss, gd_loss, g_optim],\n",
    "                                                             {t_input_gen: xgenin, t_target_image: xt,\n",
    "                                                              t_input_mask: xm})\n",
    "                print(\n",
    "                    \"Epoch [%2d/%2d] [%4d/%4d] [%4d/%4d]: d_loss: %.8f g_loss: %.8f (mse: %.6f gdl: %.6f adv: %.6f)\" % (\n",
    "                        j, epochs + val_restore, i, iterations_train, k, div_patches - 1, errd, errg, errmse, errgd,\n",
    "                        errgan))\n",
    "\n",
    "                # ========================= evaluate & save model ========================= #\n",
    "\n",
    "                if k == 1 and i % 20 == 0:\n",
    "                    if j - val_restore == 0:\n",
    "                        x_true_img = xt[0]\n",
    "                        if normfactor != 0:\n",
    "                            x_true_img = ((x_true_img + 1) * normfactor)  # denormalize\n",
    "                        img_true = nib.Nifti1Image(x_true_img, np.eye(4))\n",
    "                        img_true.to_filename(\n",
    "                            os.path.join(path_prediction, str(j) + str(i) + 'true.nii.gz'))\n",
    "\n",
    "                        x_gen_img = xgenin[0]\n",
    "                        if normfactor != 0:\n",
    "                            x_gen_img = ((x_gen_img + 1) * normfactor)  # denormalize\n",
    "                        img_gen = nib.Nifti1Image(x_gen_img, np.eye(4))\n",
    "                        img_gen.to_filename(\n",
    "                            os.path.join(path_prediction, str(j) + str(i) + 'gen.nii.gz'))\n",
    "\n",
    "                    x_pred = session.run(gen_test.outputs, {t_input_gen: xgenin})\n",
    "                    x_pred_img = x_pred[0]\n",
    "                    if normfactor != 0:\n",
    "                        x_pred_img = ((x_pred_img + 1) * normfactor)  # denormalize\n",
    "                    img_pred = nib.Nifti1Image(x_pred_img, np.eye(4))\n",
    "                    img_pred.to_filename(\n",
    "                        os.path.join(path_prediction, str(j) + str(i) + '.nii.gz'))\n",
    "\n",
    "                    max_gen = np.amax(x_pred_img)\n",
    "                    max_real = np.amax(x_true_img)\n",
    "                    if max_gen > max_real:\n",
    "                        val_max = max_gen\n",
    "                    else:\n",
    "                        val_max = max_real\n",
    "                    min_gen = np.amin(x_pred_img)\n",
    "                    min_real = np.amin(x_true_img)\n",
    "                    if min_gen < min_real:\n",
    "                        val_min = min_gen\n",
    "                    else:\n",
    "                        val_min = min_real\n",
    "                    val_psnr = psnr(np.multiply(x_true_img, xm[0]), np.multiply(x_pred_img, xm[0]),\n",
    "                                    data_range=val_max - val_min)\n",
    "                    val_ssim = ssim(np.multiply(x_true_img, xm[0]), np.multiply(x_pred_img, xm[0]),\n",
    "                                    data_range=val_max - val_min, multichannel=True)\n",
    "\n",
    "        saver.save(sess=session, save_path=checkpoint_dir, global_step=step)\n",
    "        print(\"Saved step: [%2d]\" % step)\n",
    "        step = step + 1\n",
    "\n",
    "\n",
    "def evaluate(upsampling_factor, residual_blocks, feature_size, checkpoint_dir_restore, path_volumes, nn, subpixel_NN,\n",
    "             img_height, img_width, img_depth):\n",
    "    traindataset = Train_dataset(1)\n",
    "    iterations = math.ceil(\n",
    "        (len(traindataset.subject_list) * 0.2))\n",
    "    print(len(traindataset.subject_list))\n",
    "    print(iterations)\n",
    "    totalpsnr = 0\n",
    "    totalssim = 0\n",
    "    array_psnr = np.empty(iterations)\n",
    "    array_ssim = np.empty(iterations)\n",
    "    batch_size = 1\n",
    "    div_patches = 4\n",
    "    num_patches = traindataset.num_patches\n",
    "\n",
    "    # define model\n",
    "    t_input_gen = tf.placeholder('float32', [1, None, None, None, 1],\n",
    "                                 name='t_image_input_to_SRGAN_generator')\n",
    "    srgan_network = generator(input_gen=t_input_gen, kernel=3, nb=residual_blocks,\n",
    "                              upscaling_factor=upsampling_factor, feature_size=feature_size, subpixel_NN=subpixel_NN,\n",
    "                              img_height=img_height, img_width=img_width, img_depth=img_depth, nn=nn,\n",
    "                              is_train=False, reuse=False)\n",
    "\n",
    "    # restore g\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "\n",
    "    saver = tf.train.Saver(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"SRGAN_g\"))\n",
    "    saver.restore(sess, tf.train.latest_checkpoint(checkpoint_dir_restore))\n",
    "\n",
    "    for i in range(0, iterations):\n",
    "        # extract volumes\n",
    "        xt_total = traindataset.data_true(654 + i)\n",
    "        xt_mask = traindataset.mask(654 + i)\n",
    "        normfactor = (np.amax(xt_total[0])) / 2\n",
    "        x_generator = ((xt_total[0] - normfactor) / normfactor)\n",
    "        res = 1 / upsampling_factor\n",
    "        x_generator = x_generator[:, :, :, np.newaxis]\n",
    "        x_generator = gaussian_filter(x_generator, sigma=1)\n",
    "        x_generator = zoom(x_generator, [res, res, res, 1], prefilter=False)\n",
    "        xg_generated = sess.run(srgan_network.outputs, {t_input_gen: x_generator[np.newaxis, :]})\n",
    "        xg_generated = ((xg_generated + 1) * normfactor)\n",
    "        volume_real = xt_total[0]\n",
    "        volume_real = volume_real[:, :, :, np.newaxis]\n",
    "        volume_generated = xg_generated[0]\n",
    "        volume_mask = aggregate(xt_mask)\n",
    "        # compute metrics\n",
    "        max_gen = np.amax(volume_generated)\n",
    "        max_real = np.amax(volume_real)\n",
    "        if max_gen > max_real:\n",
    "            val_max = max_gen\n",
    "        else:\n",
    "            val_max = max_real\n",
    "        min_gen = np.amin(volume_generated)\n",
    "        min_real = np.amin(volume_real)\n",
    "        if min_gen < min_real:\n",
    "            val_min = min_gen\n",
    "        else:\n",
    "            val_min = min_real\n",
    "        val_psnr = psnr(np.multiply(volume_real, volume_mask), np.multiply(volume_generated, volume_mask),\n",
    "                        data_range=val_max - val_min)\n",
    "        array_psnr[i] = val_psnr\n",
    "\n",
    "        totalpsnr += val_psnr\n",
    "        val_ssim = ssim(np.multiply(volume_real, volume_mask), np.multiply(volume_generated, volume_mask),\n",
    "                        data_range=val_max - val_min, multichannel=True)\n",
    "        array_ssim[i] = val_ssim\n",
    "        totalssim += val_ssim\n",
    "        print(val_psnr)\n",
    "        print(val_ssim)\n",
    "        # save volumes\n",
    "        filename_gen = os.path.join(path_volumes, str(i) + 'gen.nii.gz')\n",
    "        img_volume_gen = nib.Nifti1Image(volume_generated, np.eye(4))\n",
    "        img_volume_gen.to_filename(filename_gen)\n",
    "        filename_real = os.path.join(path_volumes, str(i) + 'real.nii.gz')\n",
    "        img_volume_real = nib.Nifti1Image(volume_real, np.eye(4))\n",
    "        img_volume_real.to_filename(filename_real)\n",
    "\n",
    "    print('{}{}'.format('PSNR: ', array_psnr))\n",
    "    print('{}{}'.format('SSIM: ', array_ssim))\n",
    "    print('{}{}'.format('Mean PSNR: ', array_psnr.mean()))\n",
    "    print('{}{}'.format('Mean SSIM: ', array_ssim.mean()))\n",
    "    print('{}{}'.format('Variance PSNR: ', array_psnr.var()))\n",
    "    print('{}{}'.format('Variance SSIM: ', array_ssim.var()))\n",
    "    print('{}{}'.format('Max PSNR: ', array_psnr.max()))\n",
    "    print('{}{}'.format('Min PSNR: ', array_psnr.min()))\n",
    "    print('{}{}'.format('Max SSIM: ', array_ssim.max()))\n",
    "    print('{}{}'.format('Min SSIM: ', array_ssim.min()))\n",
    "    print('{}{}'.format('Median PSNR: ', np.median(array_psnr)))\n",
    "    print('{}{}'.format('Median SSIM: ', np.median(array_ssim)))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description='Predict script')\n",
    "#     parser.add_argument('-path_prediction', help='Path to save training predictions')\n",
    "#     parser.add_argument('-path_volumes', help='Path to save test volumes')\n",
    "#     parser.add_argument('-checkpoint_dir', help='Path to save checkpoints')\n",
    "#     parser.add_argument('-checkpoint_dir_restore', help='Path to restore checkpoints')\n",
    "#     parser.add_argument('-residual_blocks', default=6, help='Number of residual blocks')\n",
    "#     parser.add_argument('-upsampling_factor', default=4, help='Upsampling factor')\n",
    "#     parser.add_argument('-evaluate', default=False, help='Test the model')\n",
    "#     parser.add_argument('-subpixel_NN', default=False, help='Use subpixel nearest neighbour')\n",
    "#     parser.add_argument('-nn', default=False, help='Use Upsampling3D + nearest neighbour, RC')\n",
    "#     parser.add_argument('-feature_size', default=32, help='Number of filters')\n",
    "#     parser.add_argument('-restore', default=None, help='Checkpoint path to restore training')\n",
    "#     args = parser.parse_args()\n",
    "\n",
    "#     if args.evaluate:\n",
    "#         evaluate(upsampling_factor=int(args.upsampling_factor), feature_size=int(args.feature_size),\n",
    "#                  residual_blocks=int(args.residual_blocks), checkpoint_dir_restore=args.checkpoint_dir_restore,\n",
    "#                  path_volumes=args.path_volumes, subpixel_NN=args.subpixel_NN, nn=args.nn, img_width=224,\n",
    "#                  img_height=224, img_depth=152)\n",
    "#     else:\n",
    "#         train(upscaling_factor=int(args.upsampling_factor), feature_size=int(args.feature_size),\n",
    "#               subpixel_NN=args.subpixel_NN, nn=args.nn, residual_blocks=int(args.residual_blocks),\n",
    "#               path_prediction=args.path_prediction, checkpoint_dir=args.checkpoint_dir, img_width=128,\n",
    "#               img_height=128, img_depth=92, batch_size=1, restore=args.restore)\n",
    "\n",
    "\n",
    "path_prediction = \"training_predictions\"\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "residual_blocks = 8\n",
    "upsampling_factor = 2\n",
    "subpixel_NN = True\n",
    "feature_size = 64\n",
    "\n",
    "nn = False\n",
    "restore = None \n",
    "\n",
    "train(upscaling_factor=int(upsampling_factor), feature_size=int(feature_size),\n",
    "              subpixel_NN=subpixel_NN, nn=nn, residual_blocks=int(residual_blocks),\n",
    "              path_prediction=path_prediction, checkpoint_dir=checkpoint_dir, img_width=128,\n",
    "              img_height=128, img_depth=92, batch_size=1, restore=restore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prediction = \"training_predictions\"\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "residual_blocks = 8\n",
    "upsampling_factor = 2\n",
    "subpixel_NN = True\n",
    "feature_size = 64\n",
    "\n",
    "nn = False\n",
    "restore = None \n",
    "\n",
    "train(upscaling_factor=int(upsampling_factor), feature_size=int(feature_size),\n",
    "              subpixel_NN=subpixel_NN, nn=nn, residual_blocks=int(residual_blocks),\n",
    "              path_prediction=path_prediction, checkpoint_dir=checkpoint_dir, img_width=128,\n",
    "              img_height=128, img_depth=92, batch_size=1, restore=restore)"
   ]
  }
 ]
}